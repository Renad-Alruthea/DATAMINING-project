{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renad-Alruthea/DATAMINING-project/blob/main/Data_mining_%26_information_retriever_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAFdk5IcunK-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os._exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci-2a85_Rj_R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "directory_path = '/content/drive/MyDrive/Colab Notebooks/Document'\n",
        "TextFiles = glob.glob(f\"{directory_path}/*.txt\")\n",
        "TextTitles = [Path(text).stem for text in TextFiles]\n",
        "\n",
        "#Debug the file search\n",
        "if not TextFiles:\n",
        "    print(\"No text files found in the specified directory. Please verify the path and the files.\")\n",
        "else:\n",
        "    print(\"Text files found:\", TextFiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIdTMAlUdRxU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEDeChRJa1gh"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall numpy==1.26.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfbO-R90S2ZV"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GnyyXEhmvLi"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.utils import tokenize # This import should now work after reinstalling numpy and gensim\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "ps = PorterStemmer()\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "import string\n",
        "\n",
        "def normalize (text):\n",
        "  t = tokenize(text.lower())\n",
        "  s = [word for word in t if word not in stopwords]\n",
        "  p = [word for word in s if word not in string.punctuation]\n",
        "  st = [ps.stem(word) for word in p]\n",
        "  l = [lemmatizer.lemmatize(word) for word in st]\n",
        "  return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMfoX8dCnp2G"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXMo_RW-ji_w"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "original_docs = []\n",
        "normalized_docs = []\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "directory_path = '/content/drive/MyDrive/Colab Notebooks/Document'\n",
        "TextFiles = glob.glob(f\"{directory_path}/*.txt\")\n",
        "TextTitles = [Path(text).stem for text in TextFiles]\n",
        "for filename in os.listdir(directory_path): # Use os.listdir to get files in the directory\n",
        "    txt_file = os.path.join(directory_path, filename) # Construct the full file path\n",
        "    if os.path.isfile(txt_file) and filename.endswith(\".txt\"): # Check if it's a file and ends with .txt\n",
        "        with open(txt_file, encoding=\"utf-8\") as f: # Open the file with utf-8 encoding\n",
        "            txt_file_as_string = f.read()\n",
        "            original_docs.append(txt_file_as_string)\n",
        "            normalized_docs.append(normalize(txt_file_as_string))\n",
        "\n",
        "print(*original_docs, sep='\\n')\n",
        "print(*normalized_docs, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAJdqYcNi5s1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def counting_docs (w, docs):\n",
        "  count = 0\n",
        "  for r in range (len(docs)):\n",
        "    if (w in docs[r]):\n",
        "      count += 1\n",
        "  return (count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H9eRlzmirHW"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "def Calc_TF_IDF (document, wordDict):\n",
        "  TF_IDF = []\n",
        "  for r in range(len(document)):\n",
        "    row = []\n",
        "    for w in wordDict:\n",
        "      TF = document[r].count(w)/len(document[r])\n",
        "      row.append (round(TF*Calc_IDF(w, document),4))\n",
        "    TF_IDF.append(row)\n",
        "  return (TF_IDF)\n",
        "\n",
        "def Calc_IDF (w, documents):\n",
        "  return math.log10((len(documents)/counting_docs(w, documents)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-1KuYcPiWc_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "flat_list = []\n",
        "for sublist in normalized_docs:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)\n",
        "wordDict = list(dict.fromkeys(flat_list))\n",
        "\n",
        "TF_IDF = Calc_TF_IDF(normalized_docs, wordDict)\n",
        "# Assuming 'TextTitles' was intended instead of 'text_titles':\n",
        "df = pd.DataFrame(TF_IDF, index=TextTitles, columns=wordDict)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuvme-DwqxCN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def Calc_Q_IDF(query, docs, wordDict):\n",
        "    Q_IDF = []\n",
        "    for w in wordDict:\n",
        "      if (query.count(w) == 0):\n",
        "        Q_IDF.append(float(0))\n",
        "      else:\n",
        "        Q_IDF.append(round(query.count(w) * Calc_IDF(w, docs), 3))\n",
        "    return Q_IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hdvY2lD2M1G"
      },
      "outputs": [],
      "source": [
        "query = 'sugar'\n",
        "normalized_query = normalize (query)\n",
        "QTF = Calc_Q_IDF (normalized_query, normalized_docs, wordDict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8whtaKHfp2sp"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    mangitude_product = math.sqrt((np.dot(a, a) * np.dot(b, b)))\n",
        "    if ( mangitude_product == 0):\n",
        "      return 0\n",
        "    else:\n",
        "\t    return np.dot(a, b) /  mangitude_product\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-XEokfiyymo"
      },
      "outputs": [],
      "source": [
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "doc_sim = []\n",
        "\n",
        "for r in range(len(normalized_docs)):\n",
        "    cosine = cos_sim(QTF, TF_IDF[r])\n",
        "    if (cosine != 0):\n",
        "        doc_sim.append([TextTitles[r], cosine])\n",
        "\n",
        "if (len(doc_sim) == 0):\n",
        "    print(\"query not found\")\n",
        "else:\n",
        "    sorted_list = sorted(doc_sim, key=itemgetter(1), reverse=True)\n",
        "    print(*sorted_list, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZg_N8gwnxLR"
      },
      "outputs": [],
      "source": [
        "# 15 relevant , 15 irrelevant\n",
        "df_labeled = df.reset_index()['index'].to_frame()\n",
        "df_labeled['y'] = np.where((df_labeled['index'] == 'Sport')|(df_labeled['index'] == 'Art')|(df_labeled['index'] == 'Makeup')\n",
        "|(df_labeled['index'] == 'Business')|(df_labeled['index'] == 'Saudi Arabia')|(df_labeled['index'] == 'Travel')|(df_labeled['index'] == 'Happiness')\n",
        "|(df_labeled['index'] == 'ADHD')|(df_labeled['index'] == 'Cars')|(df_labeled['index'] == 'Environmental Pollution')|(df_labeled['index'] == 'Fashion')\n",
        "|(df_labeled['index'] == 'furniture')|(df_labeled['index'] == 'Online Learning')|(df_labeled['index'] == 'Stealing')|(df_labeled['index'] == 'Technology') , 0,1)\n",
        "df_labeled.rename(columns={\"index\": \"Document\"},inplace=True)\n",
        "df_labeled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz-u-sQynxBM"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(sorted_list, columns=['Document', 'cos_sim'])\n",
        "df_pred['y_pred'] = np.where(df_pred['cos_sim'] >= 0.05,1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZUNE4U0HpAX"
      },
      "outputs": [],
      "source": [
        "df_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msV6_8xUrU0W"
      },
      "outputs": [],
      "source": [
        "df_pred = df_pred.sort_values('Document',ascending=True)\n",
        "df_labeled = df_labeled.sort_values('Document',ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVA5PcQhdfGZ"
      },
      "outputs": [],
      "source": [
        "pd.merge(df_labeled,df_pred,on='Document')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgcNvtTEv2Si"
      },
      "outputs": [],
      "source": [
        "df2 = pd.merge(df_labeled,df_pred,on='Document').drop('cos_sim',axis=1)\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdldXf7f9N3V"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(df2['y'], df2['y_pred'])\n",
        "TN, FP, FN, TP = confusion_matrix(df2['y'], df2['y_pred']).ravel()\n",
        "print('True Positive(TP)  = ', TP)\n",
        "print('False Positive(FP) = ', FP)\n",
        "print('True Negative(TN)  = ', TN)\n",
        "print('False Negative(FN) = ', FN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLxhA254zbwI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6X5j5pJ9NoR"
      },
      "outputs": [],
      "source": [
        "Precision = TP / (TP + FP)\n",
        "Recall = TP / (TP + FN)\n",
        "print(\"Precision :\" , Precision , \"Recall: \" , Recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmfAE6-ikIHG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "file_path = '/content/BakedFoodNutritions_With_Target.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "df_copy = df.copy()\n",
        "\n",
        "\n",
        "true_labels = df_copy['Target']\n",
        "\n",
        "# حذف الأعمدة والهدف\n",
        "drop_cols = ['Food', 'ProductType', 'FlavorVariant', 'MeasureType', 'Target']\n",
        "X = df_copy.drop(columns=drop_cols)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# تطبيق خوارزمية K means ,عدد الكلاستر 2\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "predicted_clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "true_labels_encoded = le.fit_transform(true_labels)\n",
        "\n",
        "# حساب الدقة - نأخذ الأعلى بين التصنيفين\n",
        "accuracy = max(\n",
        "    accuracy_score(true_labels_encoded, predicted_clusters),\n",
        "    accuracy_score(true_labels_encoded, 1 - predicted_clusters)\n",
        ")\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# رسم بياني باستخدام PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=predicted_clusters, cmap='viridis', s=50)\n",
        "plt.title('product Clustered by Nutritional value Similarity')\n",
        "plt.xlabel('Nutritional Component 1')\n",
        "plt.ylabel('Nutritional Component 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax4LAdEwLW9l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv('/content/BakedFoodNutritions_With_Target.csv')\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBvB697QMsz8"
      },
      "outputs": [],
      "source": [
        "#frist checking for duplicated data\n",
        "\n",
        "df.duplicated().value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmRGJLCRMxTZ"
      },
      "outputs": [],
      "source": [
        "# second print the duplicated data\n",
        "\n",
        "df [df.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49JNaVrvM2QC"
      },
      "outputs": [],
      "source": [
        "#checking for null val\n",
        "\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqqiQ9eyNrss"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# تحميل البيانات\n",
        "df = pd.read_csv(\"/content/BakedFoodNutritions_With_Target.csv\")\n",
        "\n",
        "# اختيار الخصائص (features)\n",
        "features = [\n",
        "    'Calories-kcl', 'Protein-g', 'Carb-g', 'Fiber-g', 'Sugar-g',\n",
        "    'Sodium-g', 'SaturatedFat-g', 'MonounsaturatedFat-g',\n",
        "    'PolyunsaturatedFat-g', 'TransaFat-g'\n",
        "]\n",
        "X = df[features]\n",
        "\n",
        "# تحويل القيم النصية في الهدف إلى أرقام\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['Target'])  # Healthy = 0, Not Healthy = 1 (أو العكس)\n",
        "\n",
        "# تقسيم البيانات إلى تدريب واختبار\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# تدريب نموذج Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# التنبؤ بالقيم\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# حساب الدقة\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdBVzSsoN1Cf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# تحميل البيانات\n",
        "df = pd.read_csv(\"/content/BakedFoodNutritions_With_Target.csv\")\n",
        "\n",
        "# اختيار الخصائص (features)\n",
        "features = [\n",
        "    'Calories-kcl', 'Protein-g', 'Carb-g', 'Fiber-g', 'Sugar-g',\n",
        "    'Sodium-g', 'SaturatedFat-g', 'MonounsaturatedFat-g',\n",
        "    'PolyunsaturatedFat-g', 'TransaFat-g'\n",
        "]\n",
        "X = df[features]\n",
        "\n",
        "# تحويل القيم النصية في الهدف إلى أرقام\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['Target'])  # Healthy = 0, Not Healthy = 1 (أو العكس)\n",
        "\n",
        "# تقسيم البيانات إلى تدريب واختبار\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# تدريب نموذج Decision Tree\n",
        "tree_model = DecisionTreeClassifier(random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "# التنبؤ بالقيم\n",
        "y_pred = tree_model.predict(X_test)\n",
        "\n",
        "# حساب الدقة\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F3SXBwXpoVcN"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install flask flask-cors ngrok pyngrok # add pyngrok\n",
        "!pip install gensim\n",
        "# Import necessary modules\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from gensim.utils import tokenize\n",
        "import string\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import threading\n",
        "from pyngrok import ngrok # this will work now\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable Cross-Origin Resource Sharing (CORS)\n",
        "\n",
        "# Mount Google Drive for file access\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Directory path for text files\n",
        "directory_path = '/content/drive/MyDrive/Colab Notebooks/Document'\n",
        "TextFiles = glob.glob(f\"{directory_path}/*.txt\")\n",
        "TextTitles = [Path(text).stem for text in TextFiles]\n",
        "\n",
        "# Debug the file search\n",
        "if not TextFiles:\n",
        "    print(\"No text files found in the specified directory. Please verify the path and the files.\")\n",
        "else:\n",
        "    print(\"Text files found:\", TextFiles)\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize preprocessing tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "ps = PorterStemmer()\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\n",
        "# Helper functions for Information Retriever\n",
        "def normalize(text):\n",
        "    t = list(tokenize(text.lower()))\n",
        "    s = [word for word in t if word not in stopwords]\n",
        "    p = [word for word in s if word not in string.punctuation]\n",
        "    st = [ps.stem(word) for word in p]\n",
        "    l = [lemmatizer.lemmatize(word) for word in st]\n",
        "    return l\n",
        "\n",
        "# Load and normalize documents\n",
        "# Moved the normalize function definition before it's used in the loop\n",
        "original_docs = []\n",
        "normalized_docs = []\n",
        "\n",
        "for filename in os.listdir(directory_path):\n",
        "    txt_file = os.path.join(directory_path, filename)\n",
        "    if os.path.isfile(txt_file) and filename.endswith(\".txt\"):\n",
        "        with open(txt_file, encoding=\"utf-8\") as f:\n",
        "            txt_file_as_string = f.read()\n",
        "            original_docs.append(txt_file_as_string)\n",
        "            normalized_docs.append(normalize(txt_file_as_string))\n",
        "\n",
        "def counting_docs(w, docs):\n",
        "    count = 0\n",
        "    for doc in docs:\n",
        "        if w in doc:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def Calc_TF_IDF(document, wordDict):\n",
        "    TF_IDF = []\n",
        "    for r in range(len(document)):\n",
        "        row = []\n",
        "        for w in wordDict:\n",
        "            TF = document[r].count(w) / len(document[r]) if len(document[r]) > 0 else 0\n",
        "            IDF = math.log10(len(document) / counting_docs(w, document)) if counting_docs(w, document) > 0 else 0\n",
        "            row.append(round(TF * IDF, 4))\n",
        "        TF_IDF.append(row)\n",
        "    return TF_IDF\n",
        "\n",
        "def Calc_Q_IDF(query, docs, wordDict):\n",
        "    Q_IDF = []\n",
        "    for w in wordDict:\n",
        "        if query.count(w) == 0:\n",
        "            Q_IDF.append(float(0))\n",
        "        else:\n",
        "            IDF = math.log10(len(docs) / counting_docs(w, docs)) if counting_docs(w, docs) > 0 else 0\n",
        "            Q_IDF.append(round(query.count(w) * IDF, 3))\n",
        "    return Q_IDF\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10)\n",
        "\n",
        "# Build word dictionary and calculate TF-IDF\n",
        "flat_list = [item for sublist in normalized_docs for item in sublist]\n",
        "wordDict = list(dict.fromkeys(flat_list))\n",
        "TF_IDF = Calc_TF_IDF(normalized_docs, wordDict)\n",
        "\n",
        "# Load data for Classification\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Document/BakedFoodNutritions_With_Target.csv'\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File found!\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"File not found. Please verify the path.\")\n",
        "\n",
        "# Load data for Classification\n",
        "df = pd.read_csv(file_path)\n",
        "features = [\n",
        "    'Calories-kcl', 'Protein-g', 'Carb-g', 'Fiber-g', 'Sugar-g',\n",
        "    'Sodium-g', 'SaturatedFat-g', 'MonounsaturatedFat-g',\n",
        "    'PolyunsaturatedFat-g', 'TransaFat-g'\n",
        "]\n",
        "X = df[features]\n",
        "y = df['Target'].apply(lambda x: 1 if x == \"Healthy\" else 0)  # Convert labels to binary\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Define endpoint for Information Retriever\n",
        "@app.route('/search', methods=['POST'])\n",
        "def search():\n",
        "    try:\n",
        "        query = request.json.get('query')\n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Query is required\"}), 400\n",
        "\n",
        "        normalized_query = normalize(query)\n",
        "        QTF = Calc_Q_IDF(normalized_query, normalized_docs, wordDict)\n",
        "        doc_sim = []\n",
        "\n",
        "        for r in range(len(normalized_docs)):\n",
        "            cosine = cos_sim(QTF, TF_IDF[r])\n",
        "            if cosine != 0:\n",
        "                doc_sim.append([TextTitles[r], cosine])\n",
        "\n",
        "        sorted_list = sorted(doc_sim, key=lambda x: x[1], reverse=True)\n",
        "        return jsonify({\"results\": sorted_list})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Define endpoint for Classification\n",
        "@app.route('/classify', methods=['POST'])\n",
        "def classify():\n",
        "    try:\n",
        "        data = request.json\n",
        "        calories = float(data.get('calories'))\n",
        "        sugar = float(data.get('sugar'))\n",
        "        saturated_fat = float(data.get('saturated_fat'))\n",
        "\n",
        "        if not all([calories, sugar, saturated_fat]):\n",
        "            return jsonify({\"error\": \"All features are required\"}), 400\n",
        "\n",
        "        input_features = [[calories, 0, 0, 0, sugar, 0, saturated_fat, 0, 0, 0]]\n",
        "        prediction = model.predict(input_features)[0]\n",
        "        result = \"Healthy\" if prediction == 1 else \"Not Healthy\"\n",
        "\n",
        "        return jsonify({\"result\": result})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "ngrok.set_auth_token(\"2uufMlSL0ow7ob72BCxneFd6TQy_3zjkyU6EUFp8hhFbPQLEY\")  # Replace with your actual token\n",
        "\n",
        "public_url = ngrok.connect(5000)  # Reconnect ngrok\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "app.run(port=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqPoHuvjYB7n"
      },
      "outputs": [],
      "source": [
        "!curl -X POST https://e332-35-245-7-137.ngrok-free.app/search \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d '{\"query\": \"sugar\"}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpE0KId5iLf4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}